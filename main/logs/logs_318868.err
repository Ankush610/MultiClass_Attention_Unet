The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
I1120 16:51:25.964000 1611406 torch/distributed/run.py:657] Using nproc_per_node=2.
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199] Starting elastic_operator with launch configs:
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   entrypoint         : train_ddp.py
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   min_nodes          : 1
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   max_nodes          : 1
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   nproc_per_node     : 2
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   run_id             : none
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   rdzv_backend       : static
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   rdzv_endpoint      : 127.0.0.1:29500
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   rdzv_configs       : {'rank': 0, 'timeout': 900}
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   max_restarts       : 0
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   monitor_interval   : 0.1
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   log_dir            : /tmp/torchelastic_czujb6ka
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   metrics_cfg        : {}
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199]   event_log_handler  : null
I1120 16:51:25.965000 1611406 torch/distributed/launcher/api.py:199] 
I1120 16:51:25.966000 3209736 torch/distributed/run.py:657] Using nproc_per_node=2.
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199] Starting elastic_operator with launch configs:
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   entrypoint         : train_ddp.py
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   min_nodes          : 1
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   max_nodes          : 1
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   nproc_per_node     : 2
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   run_id             : none
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   rdzv_backend       : static
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   rdzv_endpoint      : 127.0.0.1:29500
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   rdzv_configs       : {'rank': 0, 'timeout': 900}
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   max_restarts       : 0
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   monitor_interval   : 0.1
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   log_dir            : /tmp/torchelastic_bkgdjlmg
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   metrics_cfg        : {}
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199]   event_log_handler  : null
I1120 16:51:25.967000 3209736 torch/distributed/launcher/api.py:199] 
I1120 16:51:25.967000 1611406 torch/distributed/elastic/agent/server/api.py:869] [default] starting workers for entrypoint: python3
I1120 16:51:25.968000 1611406 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
I1120 16:51:25.969000 3209736 torch/distributed/elastic/agent/server/api.py:869] [default] starting workers for entrypoint: python3
I1120 16:51:25.969000 3209736 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523] [default] Rendezvous complete for workers. Result:
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   restart_count=0
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   master_addr=127.0.0.1
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   master_port=29500
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   group_rank=0
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   group_world_size=1
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   local_ranks=[0, 1]
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   role_ranks=[0, 1]
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   global_ranks=[0, 1]
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   role_world_sizes=[2, 2]
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   global_world_sizes=[2, 2]
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523]   event_log_handler=null
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:523] 
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group
I1120 16:51:26.100000 1611406 torch/distributed/elastic/agent/server/local_elastic_agent.py:296] use_agent_store: True
I1120 16:51:26.101000 1611406 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I1120 16:51:26.101000 1611406 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523] [default] Rendezvous complete for workers. Result:
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   restart_count=0
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   master_addr=127.0.0.1
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   master_port=29500
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   group_rank=0
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   group_world_size=1
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   local_ranks=[0, 1]
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   role_ranks=[0, 1]
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   global_ranks=[0, 1]
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   role_world_sizes=[2, 2]
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   global_world_sizes=[2, 2]
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523]   event_log_handler=null
I1120 16:51:26.101000 3209736 torch/distributed/elastic/agent/server/api.py:523] 
I1120 16:51:26.102000 3209736 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group
I1120 16:51:26.102000 3209736 torch/distributed/elastic/agent/server/local_elastic_agent.py:296] use_agent_store: True
I1120 16:51:26.102000 3209736 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I1120 16:51:26.102000 3209736 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 160, in <module>
[rank1]:     (train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 30, in load_data
[rank1]:     train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=split_num, random_state=42)
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 208, in wrapper
[rank1]:     validate_parameter_constraints(
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
[rank1]:     raise InvalidParameterError(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[rank1]: sklearn.utils._param_validation.InvalidParameterError: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 160, in <module>
[rank0]:     (train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 30, in load_data
[rank0]:     train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=split_num, random_state=42)
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 208, in wrapper
[rank0]:     validate_parameter_constraints(
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
[rank0]:     raise InvalidParameterError(
[rank0]: sklearn.utils._param_validation.InvalidParameterError: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 160, in <module>
[rank0]:     (train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 30, in load_data
[rank0]:     train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=split_num, random_state=42)
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 208, in wrapper
[rank0]:     validate_parameter_constraints(
[rank0]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
[rank0]:     raise InvalidParameterError(
[rank0]: sklearn.utils._param_validation.InvalidParameterError: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 160, in <module>
[rank1]:     (train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/main/train_ddp.py", line 30, in load_data
[rank1]:     train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=split_num, random_state=42)
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 208, in wrapper
[rank1]:     validate_parameter_constraints(
[rank1]:   File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 98, in validate_parameter_constraints
[rank1]:     raise InvalidParameterError(
[rank1]: sklearn.utils._param_validation.InvalidParameterError: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0 instead.
[rank0]:[W1120 16:52:10.436277466 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1120 16:52:10.525609634 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1120 16:52:20.699000 1611406 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1611432 closing signal SIGTERM
W1120 16:52:20.738000 3209736 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3209760 closing signal SIGTERM
E1120 16:52:20.742000 3209736 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 3209761) of binary: /home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/bin/python3
Traceback (most recent call last):
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/bin/accelerate", line 10, in <module>
    sys.exit(main())
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_ddp.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-20_16:52:20
  host      : rpgpu019.iuac.res.in
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3209761)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E1120 16:52:20.764000 1611406 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1611431) of binary: /home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/bin/python3
Traceback (most recent call last):
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/bin/accelerate", line 10, in <module>
    sys.exit(main())
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/omjadhav/ankush/medical_project/Multiclass-Segmentation-in-PyTorch/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_ddp.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-20_16:52:20
  host      : rpgpu020.iuac.res.in
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1611431)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: rpgpu019: task 0: Exited with exit code 1
srun: error: rpgpu020: task 1: Exited with exit code 1
